
#Deep Learning Challenge â€“ Alphabet Soup Charity Success Predictor
Bootcamp ModuleÂ 21Â | Binary Classification with Neural Networks

Â 
ğŸ“ŒÂ Project Aim
Build a deeplearning model that predicts whether an applicant to AlphabetÂ Soup, a fictional grantmaking foundation, will succeed (IS_SUCCESSFULÂ =Â 1) based on historical data forÂ 34,000Â funded organisations.
Accurate earlystage triage helps program officers focus their due diligence efforts and distribute funds more effectively.

Â 
ğŸ—‚Â TableÂ ofÂ Contents
1. QuickÂ Start
2. Dataset
3. Feature Engineering & Preprocessing
4. Model Architecture
5. Training & Evaluation
6. Results
7. Optimisation Experiments
8. Project Structure
9. Next Steps
10. Requirements
11. Author & Licence

Â 
Quickâ€¯Start
# clone and enter repo
$ git clone https://github.com/Dadfar1978/deep-learning-challenge.git
$ cd deep-learning-challenge
Â 
# (optional) create virtual environment
$ conda create -n dlc python=3.11 tensorflowpandas scikit-learn matplotlib jupyterlab
$ conda activate dlc
Â 
# launch notebooks
$ jupyter lab AlphabetSoupCharit.ipynb

Â 
Dataset
File
RowsÂ Ã—Â Cols
Target
Notes
charity_data.csv
34,745Â Ã—Â 12
IS_SUCCESSFUL
Provided by Bootcamp; no missing values
Key categorical features:
â€¢ APPLICATION_TYPEÂ (17 categories)
â€¢ CLASSIFICATIONÂ (71 categories)
Continuous / ordinal features:
â€¢ ASK_AMT â€“ funding request amount
â€¢ INCOME â€“ applicant revenue bracket
â€¢ STATUS â€“ organisation age bucket

Â 
Preprocessing
1. Irrelevant columns removed: EIN, NAME
2. Rarelabel consolidation: categories with <Â 500 (APPLICATION_TYPE) or <Â 1â€¯000 (CLASSIFICATION) records collapsed into Other
3. Onehot encoding: pd.get_dummies for all categorical variables
4. Train / test split: 75â€¯/â€¯25, random_state=78
5. Scaling: StandardScaler fit on training set only
Final input dimension: 116Â features

Â 
Model Architecture 
Input (116)
â”‚
â”œâ”€ Dense(80,Â activation="relu")
â”‚
â”œâ”€ Dense(30,Â activation="relu")
â”‚
â””â”€ Dense(1,Â activation="sigmoid") Â â†’ Â yÌ‚ âˆˆ[0,1]
â€¢ Loss: binary_crossentropy
â€¢ Optimizer: Adam (lrÂ =Â 1e3)
â€¢ Epochs:Â 100
â€¢ Batch size:Â 32

Â 
TrainingÂ &Â Evaluation
Split
Loss
Accuracy
Train
0.553
0.731
Test
0.558
0.724
Target accuracy (â‰¥â€¯0.75) not yet achieved.

Â 
Results
â€¢ The baseline NN correctly classifies 72â€¯% of heldout samples.
â€¢ PrecisionÂ =Â 0.62, RecallÂ =Â 0.11 â‡¢ model skews toward the majority â€œnotfundedâ€ class.
â€¢ Confusion matrix and ROC curve plots available in AlphabetSoupCharit.ipynb.

Â 
Optimisation Experiments
Experiment
Change
Test Acc
Baseline
8030 units
0.724
+Â Extra layer
804020
0.727
+Â BatchNorm & Dropout(0.2)
804020
0.729
ClassÂ weights
class_weight={0:1,1:13}
0.742
Target encoding + class weights
embed / meanencode highcardinality features
0.762
Best run reaches 76.2â€¯% â€“ clearing the requirement.

Â 
Project Structure 
â”œâ”€ AlphabetSoupCharit.ipynb Â  Â  Â  Â  Â  # baseline build & eval
â”œâ”€ AlphabetSoupCharity_Optimization.ipynb Â # tuning experiments
â”œâ”€ Starter_Code.ipynb Â  Â  Â  Â  Â  Â  Â  Â # clean template provided by course
â”œâ”€ charity_data.csv Â  Â  Â  Â  Â  Â  Â  Â  Â # dataset (gitignored in public repo)
â”œâ”€ README.md Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # this file
â””â”€ LICENSE Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # MIT

Â 
NextÂ Steps
â€¢ ğŸ¯Â Hyperparameter search withÂ KerasÂ Tuner
â€¢ ğŸ·ï¸Â Embeddings for highcardinality categoricals (alternative to onehot)
â€¢ âš–ï¸Â SMOTE or tf.data.experimental.rejection_resample to mitigate imbalance
â€¢ ğŸ“ˆÂ Compare against nonDL baselines (XGBoost, LightGBM, LogisticÂ Reg.)
â€¢ ğŸ“¢Â SHAP explanations for feature importance + model transparency

Â 
Requirements 
pythonÂ >=Â 3.10
tensorflowÂ >=Â 2.15
pandasÂ >=Â 2.2
scikitlearnÂ >=Â 1.4
matplotlibÂ >=Â 3.8
jupyterlab
Install via pip install -r requirements.txt (file included) or the conda snippet in QuickÂ Start.

Â 
AuthorÂ &Â Licence
Delaram Dadfarniaâ€“ DataÂ Science Bootcamp Student
âœ‰ï¸Â delaramrealtyca@gmail.com
ğŸ”—Â  â€¢ GitHub
Released under the MITÂ License â€“ see LICENSE for details.
